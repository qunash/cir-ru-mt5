{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def cyrillic_to_latin(text):\n",
    "    with open('../data/kbd cyrillic-latin alphabet table.txt', 'r', encoding='utf-8') as alphabet_table:\n",
    "        for line in alphabet_table:\n",
    "            key, value = line.split(':')\n",
    "            text = text.replace(key, value.replace('\\n', ''))\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def create_dataset(path_to_books, path_to_dataset):\n",
    "    with open(path_to_dataset, 'w', encoding='utf8') as f:\n",
    "        for filename in os.listdir(path_to_books):\n",
    "            if filename.startswith('kbd-ru'):\n",
    "                with open(os.path.join(path_to_books, filename), 'r', encoding='utf8') as f_in:\n",
    "                    for line in f_in:\n",
    "                        line_kbd, line_ru = line.strip().split('ðŸ˜€')\n",
    "                        json.dump({ \"translation\": { \"kbd\": cyrillic_to_latin(line_kbd), \"ru\": line_ru } }, f, ensure_ascii=False, sort_keys=True)\n",
    "                        f.write(\"\\n\")\n",
    "\n",
    "create_dataset('../data/books/', '../data/books/kbd_lat-ru_dataset.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-180651606494e575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to C:\\Users\\anzor\\.cache\\huggingface\\datasets\\json\\default-180651606494e575\\0.0.0\\c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8acb88a0014144ad8109ca22c1932b76"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3070e8e01315447bb8179ee154100de0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:\\Users\\anzor\\.cache\\huggingface\\datasets\\json\\default-180651606494e575\\0.0.0\\c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e13b090d9df34b9ca07aa4122ff012d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 19377\n    })\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 510\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 510\n    })\n})"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files='../data/books/kbd_lat-ru_dataset.jsonl')\n",
    "dataset = dataset['train']\n",
    "# dataset = dataset.train_test_split(test_size=0.05, shuffle=True)\n",
    "\n",
    "# 95% train, 5% test + validation\n",
    "train_test_valid = dataset.train_test_split(test_size=0.05)\n",
    "# Split the 5% test + valid in half test, half valid\n",
    "test_valid = train_test_valid['test'].train_test_split(test_size=0.5)\n",
    "# gather everyone if you want to have a single DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test_valid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'validation': test_valid['train']})\n",
    "\n",
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n",
      "The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0c2b0144cf9d4841b5334358869f1ca4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split test to the Hub.\n",
      "The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8c499da9d16f460ba3dae43fe470b5d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split validation to the Hub.\n",
      "The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25912c7c7bb343fc96b38f3e2399093d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.push_to_hub('anzorq/kbd_lat-ru')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}