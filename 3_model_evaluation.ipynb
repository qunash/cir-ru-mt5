{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"text\", data_files={\"train\": 'data/merged_delete_later.txt'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# eval_dataset = load_dataset('opus100', 'en-ru', split='train')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "model_path = 'C:/tmp/tst-translation/kbd_lat-ru_t5-base_char_tok/checkpoint-4000'\n",
    "# model_path = 'google/mt5-small'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "# tokenizer = T5TokenizerFast.from_pretrained('kbd-ru-t5-small', extra_ids=0)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path, extra_ids=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "def translate(input_text):\n",
    "    # from bleu_eval import sacre_bleu_score\n",
    "\n",
    "    # # input_text, target_text = dataset['train'][random.randint(0, len(dataset['train']) - 1)]['text'].split('üòÄ')\n",
    "    #\n",
    "    # # target_text = 'test'\n",
    "    # # eval_lines = re.split('(?<=[.!?]) +', eval_dataset['train'][random.randint(0, len(eval_dataset['train']) - 1)]['text'])\n",
    "    # # input_text = eval_lines[random.randint(0, len(eval_lines)-1)]\n",
    "    # eval_lines = eval_dataset[random.randint(0, len(eval_dataset) - 1)]['translation']\n",
    "    # input_text =eval_lines['ru']\n",
    "    # # target_text =eval_lines['en']\n",
    "\n",
    "    # input_text = \"–º—É–∂—á–∏–Ω–∞ —É—à–µ–ª –≤ –≥–æ—Ä–æ–¥.\"\n",
    "    # target_text = \"When I looked again I noticed something shiny.\"\n",
    "    encodings = tokenizer(\n",
    "        'kbd->ru: ' + input_text,\n",
    "        # 'ru->kbd: ' + input_text,\n",
    "        # + \"</s>\",\n",
    "        padding='longest',\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\")\n",
    "\n",
    "    encodings.to(device)\n",
    "    input_ids = encodings.input_ids\n",
    "\n",
    "    # input_ids = torch.tensor([tokenizer.convert_tokens_to_ids(['‚ñÅkb', 'd', '-', '>', 'ru', '>', '‚ñÅ', '‚ñÅ', 'u…ô', '</s>'])], device=device)\n",
    "    #\n",
    "    # print(input_ids)\n",
    "\n",
    "    input_ids.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "    n_beams = 2\n",
    "    # for i in range(1, n_beams + 1):\n",
    "    output_tokens = model.generate(input_ids,\n",
    "                                   num_beams=n_beams,\n",
    "                                   num_return_sequences=n_beams,\n",
    "                                   # output_attentions=True,\n",
    "                                   # output_scores = True,\n",
    "                                   # repetition_penalty = 5.5,\n",
    "                                   # length_penalty=0.2,\n",
    "                                   # do_sample=True,\n",
    "                                   # temperature=0.1,\n",
    "                                   # bos_token_id = 2,\n",
    "                                   # output_scores=True,\n",
    "                                   # max_length=128\n",
    "                                   )\n",
    "\n",
    "    # output_tokens[output_tokens==0]=1 # replace <unk> with <s> (‚Äî bos token)\n",
    "\n",
    "    # print(str(i) + ' ' + input_text + '  ->  ' + tokenizer.decode(output_tokens[0], skip_special_tokens=True))\n",
    "    # print('n_beams: ' + str(i))\n",
    "    print('n_beams: ' + str(n_beams))\n",
    "    print('input: ' + input_text + '\\n')\n",
    "    print(tokenizer.convert_ids_to_tokens(tokenizer.encode(input_text)))\n",
    "    for token_set in output_tokens:\n",
    "        prediction = tokenizer.decode(token_set, skip_special_tokens=True)\n",
    "        # print(token_set)\n",
    "        print(\"output: {}\".format(prediction))\n",
    "        # print('BLEU score: ' + str(sacre_bleu_score(predictions=prediction, references=target_text)['score']) + '\\n')\n",
    "    # print('reference: ' + target_text)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "T5ForConditionalGeneration(\n  (shared): Embedding(8000, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(8000, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(8000, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=8000, bias=False)\n)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration\n",
    "\n",
    "# model = MT5ForConditionalGeneration.from_pretrained('models/' + model_path + '/epoch_9')\n",
    "model = T5ForConditionalGeneration.from_pretrained('C:/tmp/tst-translation')\n",
    "model.to(device)\n",
    "\n",
    "# model.config.vocab_size = len(tokenizer.get_vocab())  # remove later, bug fixing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def cyrillic_to_latin(text):\n",
    "    with open('../data/kbd cyrillic-latin alphabet table.txt', 'r', encoding='utf-8') as alphabet_table:\n",
    "        for line in alphabet_table:\n",
    "            key, value = line.split(':')\n",
    "            text = text.replace(key, value.replace('\\n', ''))\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_beams: 2\n",
      "input: ·πØuri yogupsƒ±s\n",
      "\n",
      "['‚ñÅ', '·πØ', 'u', 'r', 'i', '‚ñÅ', 'y', 'o', 'g', 'u', 'p', 's', 'ƒ±', 's', '</s>']\n",
      "output: –∫<extra_id_-557>j<extra_id_-1148><extra_id_-53>—Ü–∞<extra_id_-207>u<extra_id_-175><extra_id_-186><extra_id_-1369>√§<extra_id_-6529><extra_id_-1881>t –∫<extra_id_-852>\n",
      "output: –∫<extra_id_-557>j<extra_id_-1148><extra_id_-53>—Ü–∞<extra_id_-207>u<extra_id_-175><extra_id_-186><extra_id_-1369>√§<extra_id_-6529><extra_id_-1881>t –∫Œ±\n"
     ]
    }
   ],
   "source": [
    "inp = '–ø—Å—ã –¥–∞—Ö—ç—Ö—ç—Ä'\n",
    "# inp = '—Å—ç —É–Ω—ç –¥–∞—Ö—ç —Å–∏I—ç—â.'\n",
    "# inp = '—É–∏ —É–Ω—ç—Ä –¥–∞—Ö—ç—â'\n",
    "# inp = '–º—ã –ø—Å—ã—Ä –ø—Å—ã–Ω—âI—ç—â.'\n",
    "# inp = '—âI–∞–ª—ç–º —Å–∞–±–∏–π—Ä –∫—ä–∏I—ç—Ç–∞—â'\n",
    "# inp = '—âI–∞–ª—ç—Ä –≥—É–±–∑—ã–≥—ä—ç—â'\n",
    "# inp = '–∏–≥—ä—ç—âI—ç–≥—ä—É–∞—â'\n",
    "# inp = '—É–∫—ä—ç–∫I—É—ç–Ω—É—â'\n",
    "# inp = '–ø—Å–∞–ª—ä—ç –¥–∞—Ö—ç—Ö—ç—Ä'\n",
    "# inp = '–ø—Å–∞–ª—ä—ç –Ω—ã–∫—ä—É—ç'\n",
    "# inp = '–ø—Å—ã—à—Ö—É—ç'\n",
    "# inp = '—É–Ω—ç—Ä –ø—Å—ã—Ñ—â'\n",
    "# inp = '–∫—ä—ç–∫I—É—ç–Ω—É—Ä –∫—ä–∞–∫I—É—ç—Ä–∏ –∫—ä—ç–∫I—É–∞—Ö—ç—â –∫—ä—ç–≤–≥—ä–∞–∫I—É—ç –∫—ä—ç–≤–≥—ä—ç–∫I—É–∞—â –∫—ä—ç–∑–≥—ä—ç–∫I—É–∞—â –∫—ä—ç–¥–≥—ä—ç–∫I—É–∞—â'\n",
    "# inp = '–¥–∂–∞–Ω—ç—Ä —âI–∏—ÇI—ç–≥—ä–∞—â.'\n",
    "inp = '—É—ç—à—Ö I—ç—ÑI'\n",
    "# inp = '—à—ã –ø–ª—ä—ã–∂—å'\n",
    "# inp = '–ø–ª—ä—ã–∂—å—ã—à—Ö—É—ç'\n",
    "# inp = '–º—ã —É–Ω—ç—Ä –ø–ª—ä–∞–≥—ä—É—Ä—ç?'\n",
    "# inp = '—É–Ω—ç—Ä —Å–ª—ä—ç–≥—ä—É–∞—â'\n",
    "# inp = '—Ç—Ö—ã–ª—ä—ã—à—Ö—É—ç'\n",
    "# inp = '—Ç—Ö—ã–ª—ä —ÜI—ã–∫I—É–º —É–µ–¥–∂–∞—â'\n",
    "# inp = '—Ç—Ö—ã–ª—ä —ÜI—ã–∫I—É–Ω–∏—ÇI—ç'\n",
    "# inp = '–∂—ã–≥ –¥–∞—Ö—ç'\n",
    "# inp = '–º–∞—Ö—É—ç, –º–∞–∑—ç, –≥—ä—ç'\n",
    "# inp = '–º–∞—Ö—É—ç, –º–∞–∑—ç, –∏–ª—ä—ç—Å'\n",
    "# inp = '—Å—ã—Ç –∞–ø—Ö—É—ç–¥—ç—É —âI—ã–∂—ã–øI—ç—Ä?'\n",
    "# inp = '–∞–ø—Ö—É—ç–¥—ç—É —â—Ö—å—ç –∂—ã–øI—ç—Ä—ç?'\n",
    "# inp = '—Å—ã—Ç?'\n",
    "# inp = '—Ö—É–∏—Ç –∏—âI–∞—â'\n",
    "# inp = '–ö—ä—ã–∫I—ç–ª—ä—ã–∫I—É—ç–Ω—É—â'\n",
    "# inp = '–∞—Ä –∫—ä–∞–∫I—É—ç—Ä–∏ –ø—Å—ç–ª—ä–∞—â'\n",
    "# inp = '–ø—â—ç–¥–µ–π –∫—ä—ã–∑—ç–ø—Ç—ã–∂–º—ç, —Ç—Ö—ã–ª—ä—ã—Ä —É—ç—Å—Ç—ã–Ω—É—â'\n",
    "# inp = '–∞—Ä –∫—ä–∞–∫I—É—ç—Ä–∏ –∂–∏I–∞—â: \"–ø—Å—ã —Å–µ—Ñ—ç–Ω—É—Ç\".'\n",
    "# inp = '–∞—Ä –∫—ä–∞–∫I—É—ç—Ä–∏ –∂–∏I–∞—â: \"–ø—Å—ã —Å–µ—Ñ—ç–Ω—É—â\".'\n",
    "# inp = '–∞—Ä –Ω—ç—Ö—ä—ã—ÑI—â'\n",
    "# inp = '–∞—Ä –Ω—ç—Ö—ä –¥–∞—Ö—ç—â'\n",
    "# inp = '–º–∞—Ö—É–∏—ÇI'\n",
    "# inp = '–±–≥—ã –ª—ä–∞–≥—ç'\n",
    "# inp = '—à—É –∑–∞–∫—ä—É—ç'\n",
    "# inp = '–ø—â–∞—â—ç —ÜI—ã–∫I—É'\n",
    "# inp = '–º—ç–ª —Ö—É–∂—å—Ö—ç—Ä'\n",
    "# inp = '–¥—ç–Ω—ç —Å—ã–∑–¥—ç–∫I—É—ç—Ä?'\n",
    "# inp = '–∑—ã –º—ç–ª –≥—É—ç—Ä'\n",
    "# inp = '—Ñ—ã—Ö—É–µ–π–º—ç —Ñ—ã–∫—ä—ç–∫I—É—ç–∂'\n",
    "# inp = '–∞–±—ã —â—ã–≥—ä—É—ç –∞—Ö—ç—Ä —Ö–∞–¥—ç–º —â—ã–ª—ç–∂—å—ç—Ä—Ç'\n",
    "# inp = '–º—ç–∑—ã–º –¥—ã–¥—ã—Ö—å–∞—â'\n",
    "# inp = '—Å–∏ –ø—â–∞—â—ç —ÜI—ã–∫I—É'\n",
    "# inp = '—É–∏ I—É—ç—Ö—É –∑–¥—ç—â—ã–º—ãI—ç —É–º—ã–∫I—É—ç'\n",
    "# inp = '—É–∏ I—É—ç—Ö—É –∑–¥—ç—â—ã–º—ãI—ç–º —É–º—ã–∫I—É—ç'\n",
    "# inp = '–¶I—ã—Ö—É—Ö—ç—Ä —è –∑–∞–∫—ä—É—ç—É –ø—Å—ç—É–∫—ä—ã–º'\n",
    "# inp = '—Ö–∞–±–∑—ç –¥–∞—Ö—ç'\n",
    "# inp = '—ÜI—ã—Ö—É–º —É–∑ –∫—ä—ã—Ö–∏—Ö—ã–Ω—ã–º —Ö—É—ç–¥—ç—É'\n",
    "# inp = '—É—ç –¥—ç–Ω—ç —É—â—ãI—ç?'\n",
    "# inp = '–¥—ã—à—Ö—ç–Ω—É–∫—ä—ç?'\n",
    "# inp = '–¥–µ–ø–ª—ä—ã–Ω—É–∫—ä—ç?'\n",
    "# inp = '—É–Ω—ç—Ö—ç—Ä –∏–Ω—â'\n",
    "# inp = '–¥—ç –∫—ä–∞–ª—ç–º –¥—ã—â–æ–ø—Å—ç—É'\n",
    "# inp = '–∞–±—ã —â—ã–≥—ä—É—ç –º—ç–∑—ã–º —Å—ã—â—ã–ø—Å—ç—É—Ä—Ç'\n",
    "# inp = '—Å—ã—à—Ö—ç—Ä–∏ —Å—ã–∂–µ–∏–∂–∞—â'\n",
    "# inp = '—Å—ã–∑–¥—ç—â—ãI—ç—Ä –¥—ç–Ω—ç?'\n",
    "# inp = '—É–∑–¥—ç—â—ãI—ç—Ä –¥—ç–Ω—ç?'\n",
    "# inp = '–∫I—É—ç–∏ –∂–µ–π'\n",
    "# inp = '–º—ç–ª—Ö—ç—Ä –º—ç–∑—ã–º —â–æ–¥–∂—ç–≥—É'\n",
    "# inp = '—Ö—å—ç—Ö—ç–º –º—ãI—ç—Ä—ã—Å—ç—Ö—ç—Ä —è—à—Ö—ã—Ä–∫—ä—ã–º'\n",
    "# inp = '–∞–¥—ã–≥—ç'\n",
    "inp = '—ÇI—É—Ä–∏ –π–æ–≥—É–ø—Å—ã—Å'\n",
    "# inp = '–∏—Ä–∞–≥—ä—ç–ª—ä–∞–≥—ä—É–Ω—É'\n",
    "\n",
    "translate(cyrillic_to_latin(inp))\n",
    "# translate(inp)\n",
    "\n",
    "# translate('—É–º–Ω—ã–π –¥–æ–º')\n",
    "# translate('—Ç–≤–æ–π –ª–µ—Å')\n",
    "# translate('–∑–∞–±–µ–∂–∞–ª –≤ –¥–æ–º')\n",
    "# translate('—Ç—ã –∫—Ç–æ?')\n",
    "# translate('—è –Ω–µ –∑–Ω–∞—é')\n",
    "# translate('–û–Ω–∏ –ª—é–±–∏–ª–∏ –¥—Ä—É–≥ –¥—Ä—É–≥–∞.')\n",
    "# translate('–∫—Ç–æ-—Ç–æ –∑–∞–±–µ–∂–∞–ª –≤ –¥–æ–º.')\n",
    "# translate('–ª–æ—à–∞–¥—å –±–µ–∂–∏—Ç —Å –≥–æ—Ä—ã')\n",
    "# translate('–ø–æ–∫—É—à–∞—Ç—å –≤–º–µ—Å—Ç–µ —Å –Ω–∞–º–∏')\n",
    "# translate('—è –∏ –æ–Ω–∏')\n",
    "# translate('–ü–µ—Ä–≤—ã–µ —Å–æ—Ä–æ–∫ –¥–Ω–µ–π —Å –Ω–∏–º –±—ã–ª –º–∞–ª—å—á–∏–∫')\n",
    "\n",
    "#upload the ru-kbd model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}