{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import sentencepiece as sentencepiece\n",
    "from transformers import MT5Tokenizer, MT5Config, MT5ForConditionalGeneration\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7d8fd473ca3f8090\n",
      "Reusing dataset text (C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e775175730a47b4aea104b77fb51987"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"text\", data_files={\"train\": ['data/merged_delete_later.txt', 'data/Haji-Murat_kbd-ru.txt']})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "max_tokenization_length = 64\n",
    "\n",
    "\n",
    "def encode(text, _tokenizer):\n",
    "    return _tokenizer(text,\n",
    "                      padding='longest',\n",
    "                      max_length=max_tokenization_length,\n",
    "                      truncation=True,\n",
    "                      return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "def encode_batch(raw_batch, _input_tokenizer, _output_tokenizer):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "\n",
    "    for translation_pair in raw_batch['text']:\n",
    "        target_text, input_text = translation_pair.split('üòÄ')\n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "\n",
    "    encoding = encode(input_texts, _input_tokenizer)\n",
    "    input_ids, input_attention_masks_ids = encoding.input_ids, encoding.attention_mask\n",
    "\n",
    "    target_encoding = encode(target_texts, _output_tokenizer)\n",
    "    target_ids, target_attention_masks_ids = target_encoding.input_ids, target_encoding.attention_mask\n",
    "\n",
    "    input_ids.to(device)\n",
    "    input_attention_masks_ids.to(device)\n",
    "    target_ids.to(device)\n",
    "    target_attention_masks_ids.to(device)\n",
    "\n",
    "    return input_ids, input_attention_masks_ids, target_ids, target_attention_masks_ids\n",
    "\n",
    "\n",
    "def get_data_generator(_dataset, _input_tokenizer, _output_tokenizer, _batch_size=32):\n",
    "    shuffled_dataset = _dataset.shuffle()[\n",
    "        'train']  #https://huggingface.co/docs/datasets/dataset_streaming.html#reshuffle-the-dataset-at-each-epoch\n",
    "    for i in range(0, len(shuffled_dataset), _batch_size):\n",
    "        raw_batch = shuffled_dataset[i:i + _batch_size]\n",
    "        yield encode_batch(raw_batch, _input_tokenizer, _output_tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Pretraining"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# def pretrain_tokenizer(input_files, vocab_size, model_prefix, save_directory):\n",
    "#     sentencepiece.SentencePieceTrainer.Train('--input=' + input_files +\n",
    "#                                              ' --model_prefix=' + model_prefix +\n",
    "#                                              ' --vocab_size=' + str(vocab_size) +\n",
    "#                                              ' --model_type=unigram'\n",
    "#                                              ' --character_coverage=1.0'\n",
    "#                                              ' --pad_id=0 --eos_id=1 --unk_id=2 --bos_id=-1'\n",
    "#                                              ' --pad_piece=<pad> --eos_piece=</s> --unk_piece=<unk>')  #--bos_piece=<s>\n",
    "#\n",
    "#     mt5_tokenizer = MT5Tokenizer.from_pretrained(model_prefix + '.model', extra_ids=0)\n",
    "#     mt5_tokenizer.save_pretrained(save_directory)\n",
    "#\n",
    "#     # delete temporary files\n",
    "#     if os.path.exists(model_prefix + '.model'):\n",
    "#         os.remove(model_prefix + '.model')\n",
    "#     if os.path.exists(model_prefix + '.vocab'):\n",
    "#         os.remove(model_prefix + '.vocab')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "input_lang = 'ru'\n",
    "output_lang = 'kbd'\n",
    "\n",
    "input_tokenizer_path = 'tokenizers/' + input_lang\n",
    "output_tokenizer_path = 'tokenizers/' + output_lang\n",
    "\n",
    "# pretrain_tokenizer(input_files='data/for_tokenizer/tokenizer_data_ru.txt', model_prefix=input_lang, vocab_size=32000,\n",
    "#                    save_directory=input_tokenizer_path)\n",
    "# pretrain_tokenizer(input_files='data/for_tokenizer/tokenizer_data_kbd.txt', model_prefix=output_lang, vocab_size=500,\n",
    "#                    save_directory=output_tokenizer_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Using pretrained"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anzor\\AppData\\Roaming\\Python\\Python38\\site-packages\\transformers\\tokenization_utils_base.py:1642: FutureWarning: Calling T5Tokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = MT5Tokenizer.from_pretrained(input_tokenizer_path + '/spiece.model', extra_ids=0)\n",
    "output_tokenizer = MT5Tokenizer.from_pretrained(output_tokenizer_path + '/spiece.model', extra_ids=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# spm = input_tokenizer\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('—Å—ç —Å–æ—âI—ç')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('—É—ç —É–æ—âI—ç')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('–∑—ç—Ä—ã—Ñ—âI—ç—â–∏')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('–¥—ã–∑—ç—Ä—ã—âI—ç–Ω—â')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('–¥—ã–∑—ç—Ä—ã—âI—ç–Ω—É—â')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('–¥—ã–∑—ç—Ä—ã—âI–∞—â')))\n",
    "# # print(spm.convert_ids_to_tokens(spm.encode('–º—ã –∑–Ω–∞–ª–∏ –¥—Ä—É–≥ –¥—Ä—É–≥–∞')))\n",
    "# # print(spm.convert_ids_to_tokens(spm.encode('–æ—Ç–∫—É–¥–∞ —Ç–µ–±–µ –∑–Ω–∞—Ç—å')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('–¥–∂—ç–¥')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('–¥–∂—ç–¥—ã–∫I—ç')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('I—É—ç—Ö—É—âI–∞–øI—ç')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('–ª—ç–∂—å–∞–øI—ç')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('–∑—ã–±–≥—ä—ç–¥—ç–ª—ä —âI—ç–Ω—ã–≥—ä—ç–ªI—Ç')))\n",
    "# print(spm.convert_ids_to_tokens(spm.encode('–õ–æ—Å-–ê–Ω–¥–∂–µ–ª–µ—Å –∫—ä–∞–ª—ç–º —â–æ–ø—Å—ç—É')))\n",
    "# print(output_tokenizer.convert_ids_to_tokens(output_tokenizer.encode('–ú—É—Ö–∞–º–º–µ–¥')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "config = MT5Config.from_pretrained(\"google/mt5-small\")\n",
    "config.vocab_size = len(input_tokenizer.get_vocab()) + len(output_tokenizer.get_vocab()) - len(\n",
    "    output_tokenizer.special_tokens_map)  # subtract special tokens once so they're not counted twice\n",
    "config.max_length = 64"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "MT5ForConditionalGeneration(\n  (shared): Embedding(32497, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32497, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32497, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (2): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (3): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (4): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (5): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (6): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (7): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedGeluDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32497, bias=False)\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MT5ForConditionalGeneration(config)\n",
    "# model = MT5ForConditionalGeneration.from_pretrained('models/epoch_0')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Constants\n",
    "n_epochs = 30\n",
    "batch_size = 116\n",
    "print_freq = 10\n",
    "# checkpoint_freq = 2500\n",
    "lr = 1e-4\n",
    "n_batches = int(np.ceil(len(dataset['train']) / batch_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "optimizer = Adafactor(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    eps=(1e-30, 1e-3),\n",
    "    clip_threshold=1.0,\n",
    "    decay_rate=-0.8,\n",
    "    beta1=None,\n",
    "    weight_decay=0.0,\n",
    "    relative_step=False,\n",
    "    scale_parameter=False,\n",
    "    warmup_init=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir 'runs'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/30 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1fcefbf7a69c425eb97de9a5af5e8d24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0aa8621404ca46a28c71c7b2218e3d54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 0\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "924ee133d3434f7584b81e888f8a606f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 1\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7434d93b358b413b9945ed36990436ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 2\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f7b1c0157ee44f697d099b02d4c012d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 3\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "045f19c15d774cbb8db0b9ef3202a4e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 4\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad7c9a8b7b8648418c9281c21db0dcde"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 5\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d64c6858bfd40b184ca657f595e4660"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 6\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1cd7ad0b197349d48bcf19d88386155f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 7\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "593e52376ed3433aa328098adbc45f05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 8\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1e6a425725b46ab8793485fa2d43a31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 9\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ed8dea4934a4483a25b68871d96abd0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 10\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd39e53917b04e5182c9f31c22eee361"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 11\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3c5edf93c034de89dd9fca560115578"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 12\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "16ff22c5edc741198740d575f05aecd1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 13\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aafe3caea2514d60b439bf8fbf60cf6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 14\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7d07c7ed30e409f8af42031c16e5778"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 15\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ba471892b1ed4154a2ea0dd02e4364e0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 16\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d478746d6d704701af7a5f2c2b3a07ed"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 17\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "925bda6bddb64391b8d2942c40063324"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 18\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebe485cca3894aad90260828d9c2e132"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 19\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d3374b194054112875f495d250fa1db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 20\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1246b64e88b4e17b6325126c949e80b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 21\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d55553345d244dda62f37f99ef75518"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 22\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afd825cab0ec4900a5fa40f718647502"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 23\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1472b784399e45b891274b4d91775b37"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 24\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca80f277a2ee458797acd72153ab274d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 25\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aaf51e60c3984b9bb8e728cab4b14020"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 26\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb3bb945f4d64bbb9377054615867fa3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 27\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13acdea95e7a41f995cc1a3a7a84446c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 28\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/51 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1136e90266014617ac58f8ca255f5856"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at C:\\Users\\anzor\\.cache\\huggingface\\datasets\\text\\default-7d8fd473ca3f8090\\0.0.0\\e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5\\cache-96c491c21127f319.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model for epoch 29\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "losses = []\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "pbar = tqdm(range(0, n_epochs), total=n_epochs)\n",
    "for epoch_idx in pbar:\n",
    "\n",
    "    # Get data batch\n",
    "    data_generator = get_data_generator(dataset, input_tokenizer, output_tokenizer, batch_size)\n",
    "\n",
    "    for batch_idx, (input_batch, input_attention_mask_batch, target_batch, target_attention_mask_batch) in tqdm(\n",
    "            enumerate(data_generator), total=n_batches):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        model_out = model.forward(\n",
    "            input_ids=input_batch.to(device),\n",
    "            labels=target_batch.to(device),\n",
    "            attention_mask=input_attention_mask_batch.to(device),\n",
    "            decoder_attention_mask=target_attention_mask_batch.to(device)\n",
    "        )\n",
    "\n",
    "        # Calculate loss and update weights\n",
    "        loss = model_out.loss\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "\n",
    "        # Print training update info\n",
    "        if (batch_idx + 1) % print_freq == 0:\n",
    "            avg_loss = np.mean(losses[-print_freq:])\n",
    "            writer.add_scalar('Loss/train', avg_loss, batch_idx + 1 + epoch_idx * n_batches)\n",
    "            # writer.add_scalar('lr/train', scheduler.get_last_lr()[0], batch_idx + 1 + epoch_idx * n_batches)\n",
    "            writer.add_scalar('lr/train', AdafactorSchedule(optimizer).get_lr()[0],\n",
    "                              batch_idx + 1 + epoch_idx * n_batches)\n",
    "\n",
    "        pbar.set_description(\"loss = %s\" % loss.item())\n",
    "\n",
    "    print('Saving model for epoch %d' % epoch_idx)\n",
    "    model.save_pretrained('models/epoch_%d' % epoch_idx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}